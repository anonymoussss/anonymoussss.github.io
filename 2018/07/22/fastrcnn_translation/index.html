<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="deep learning,translation," />





  <link rel="alternate" href="/atom.xml" title="anonymoussss's Blog" type="application/atom+xml" />






<meta name="description" content="摘要本文提出了一种快速的基于区域的卷积网络方法（fast R-CNN）用于目标检测。Fast R-CNN建立在以前使用的深卷积网络有效地分类目标的成果上。相比于之前的成果，Fast R-CNN采用了多项创新提高训练和测试速度来提高检测精度。Fast R-CNN训练非常深的VGG16网络比R-CNN快9倍，测试时间快213倍，并在PASCAL VOC上得到更高的精度。与SPPnet相比，fast R">
<meta name="keywords" content="deep learning,translation">
<meta property="og:type" content="article">
<meta property="og:title" content="📜论文翻译 Fast R-CNN">
<meta property="og:url" content="http://yoursite.com/2018/07/22/fastrcnn_translation/index.html">
<meta property="og:site_name" content="anonymoussss&#39;s Blog">
<meta property="og:description" content="摘要本文提出了一种快速的基于区域的卷积网络方法（fast R-CNN）用于目标检测。Fast R-CNN建立在以前使用的深卷积网络有效地分类目标的成果上。相比于之前的成果，Fast R-CNN采用了多项创新提高训练和测试速度来提高检测精度。Fast R-CNN训练非常深的VGG16网络比R-CNN快9倍，测试时间快213倍，并在PASCAL VOC上得到更高的精度。与SPPnet相比，fast R">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/07/22/fastrcnn_translation/1.png">
<meta property="og:image" content="http://yoursite.com/2018/07/22/fastrcnn_translation/2.png">
<meta property="og:image" content="http://yoursite.com/2018/07/22/fastrcnn_translation/3.png">
<meta property="og:image" content="http://yoursite.com/2018/07/22/fastrcnn_translation/4.png">
<meta property="og:image" content="http://yoursite.com/2018/07/22/fastrcnn_translation/5.png">
<meta property="og:image" content="http://yoursite.com/2018/07/22/fastrcnn_translation/6.png">
<meta property="og:image" content="http://yoursite.com/2018/07/22/fastrcnn_translation/7.png">
<meta property="og:image" content="http://yoursite.com/2018/07/22/fastrcnn_translation/8.png">
<meta property="og:image" content="http://yoursite.com/2018/07/22/fastrcnn_translation/9.png">
<meta property="og:image" content="http://yoursite.com/2018/07/22/fastrcnn_translation/10.png">
<meta property="og:updated_time" content="2018-07-23T23:29:52.404Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="📜论文翻译 Fast R-CNN">
<meta name="twitter:description" content="摘要本文提出了一种快速的基于区域的卷积网络方法（fast R-CNN）用于目标检测。Fast R-CNN建立在以前使用的深卷积网络有效地分类目标的成果上。相比于之前的成果，Fast R-CNN采用了多项创新提高训练和测试速度来提高检测精度。Fast R-CNN训练非常深的VGG16网络比R-CNN快9倍，测试时间快213倍，并在PASCAL VOC上得到更高的精度。与SPPnet相比，fast R">
<meta name="twitter:image" content="http://yoursite.com/2018/07/22/fastrcnn_translation/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/07/22/fastrcnn_translation/"/>





  <title>📜论文翻译 Fast R-CNN | anonymoussss's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">anonymoussss's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/22/fastrcnn_translation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JL Ma">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="anonymoussss's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">📜论文翻译 Fast R-CNN</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-22T16:20:23+08:00">
                2018-07-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> read numbers
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文提出了一种快速的基于区域的卷积网络方法（fast R-CNN）用于目标检测。Fast R-CNN建立在以前使用的深卷积网络有效地分类目标的成果上。相比于之前的成果，Fast R-CNN采用了多项创新提高训练和测试速度来提高检测精度。Fast R-CNN训练非常深的VGG16网络比R-CNN快9倍，测试时间快213倍，并在PASCAL VOC上得到更高的精度。与SPPnet相比，fast R-CNN训练VGG16网络比他快3倍，测试速度快10倍，并且更准确。Fast R-CNN的Python和C ++（使用Caffe）实现以MIT开源许可证发布在：<a href="https://github.com/rbgirshick/fast-rcnn。" target="_blank" rel="noopener">https://github.com/rbgirshick/fast-rcnn。</a></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>　　最近，深度卷积网络<sup>1 2</sup>已经显著提高了图像分类<sup>1</sup>和目标检测<sup>3 4</sup>的准确性。与图像分类相比，目标检测是一个更具挑战性的任务，需要更复杂的方法来解决。由于这种复杂性，当前的方法（例如，<sup>3 5 4 6</sup>）采用多级流水线的方式训练模型，既慢且精度不高。<br>　　复杂性的产生是因为检测需要目标的精确定位，这就导致两个主要的难点。首先，必须处理大量候选目标位置（通常称为“提案”）。 第二，这些候选框仅提供粗略定位，其必须被精细化以实现精确定位。 这些问题的解决方案经常会影响速度，准确性或简单性。<br>　　在本文中，我们简化了最先进的基于卷积网络的目标检测器的训练过程<sup>3 5</sup>。我们提出一个单阶段训练算法，联合学习候选框分类和修正他们的空间位置。<br>　　所得到的方法用来训练非常深的检测网络（例如VGG16） 比R-CNN快9倍，比SPPnet快3倍。在运行时，检测网络在PASCAL VOC 2012数据集上实现最高准确度，其中mAP为66％（R-CNN为62％），每张图像处理时间为0.3秒，不包括候选框的生成（所有的时间都是使用一个超频到875MHz的Nvidia K40 GPU测试的）。</p>
<h3 id="R-CNN与SPPnet"><a href="#R-CNN与SPPnet" class="headerlink" title="R-CNN与SPPnet"></a>R-CNN与SPPnet</h3><p>基于区域的卷积网络方法（RCNN）通过使用深度卷积网络来分类目标候选框，获得了很高的目标检测精度。然而，R-CNN具有显着的缺点：</p>
<ul>
<li>训练过程是多级流水线。R-CNN首先使用目标候选框对卷积神经网络使用log损失进行微调。然后，它将卷积神经网络得到的特征送入SVM。 这些SVM作为目标检测器，替代通过微调学习的softmax分类器。 在第三个训练阶段，学习检测框回归。</li>
<li>训练在时间和空间上是的开销很大。对于SVM和检测框回归训练，从每个图像中的每个目标候选框提取特征，并写入磁盘。对于非常深的网络，如VGG16，这个过程在单个GPU上需要2.5天（VOC07 trainval上的5k个图像）。这些特征需要数百GB的存储空间。</li>
<li>目标检测速度很慢。在测试时，从每个测试图像中的每个目标候选框提取特征。用VGG16网络检测目标每个图像需要47秒（在GPU上）。</li>
</ul>
<p>　　R-CNN很慢是因为它为每个目标候选框进行卷积神经网络正向传递，而不共享计算。SPPnet<sup>5</sup>通过共享计算加速R-CNN。SPPnet<sup>5</sup>计算整个输入图像的卷积特征图，然后使用从共享特征图提取的特征向量来对每个候选框进行分类。通过最大池化将候选框内的特征图转化为固定大小的输出（例如，6X6）来提取针对候选框的特征。多个输出被池化，然后连接成空间金字塔池<sup>7</sup>。SPPnet在测试时将R-CNN加速10到100倍。由于更快的候选框特征提取训练时间也减少3倍。<br>　　SPP网络也有显著的缺点。像R-CNN一样，训练过程是一个多级流水线，涉及提取特征，使用log损失对网络进行微调，训练SVM分类器，最后拟合检测框回归。特征也写入磁盘。但与R-CNN不同，在<sup>5</sup>中提出的微调算法不能更新在空间金字塔池之前的卷积层。不出所料，这种限制（固定的卷积层）限制了深层网络的精度。</p>
<h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><p>我们提出一种新的训练算法，修正R-CNN和SPPnet的缺点，同时提高其速度和准确性。因为它能比较快地进行训练和测试，我们称之为Fast R-CNN。Fast RCNN方法有以下几个优点：</p>
<ul>
<li>比R-CNN和SPPnet具有更高的目标检测精度（mAP）。</li>
<li>训练是使用多任务损失的单阶段训练。</li>
<li>训练可以更新所有网络层参数。</li>
<li>不需要磁盘空间缓存特征。</li>
</ul>
<p>Fast R-CNN使用Python和C++(Caffe8)语言编写，以MIT开源许可证发布在：<a href="https://github.com/rbgirshick/fast-rcnn。" target="_blank" rel="noopener">https://github.com/rbgirshick/fast-rcnn。</a></p>
<h2 id="Fast-R-CNN架构与训练"><a href="#Fast-R-CNN架构与训练" class="headerlink" title="Fast R-CNN架构与训练"></a>Fast R-CNN架构与训练</h2><p>Fast R-CNN的架构如下图（图1）所示：<br><img src="/2018/07/22/fastrcnn_translation/1.png" alt="图１"><br>图1. Fast R-CNN架构。输入图像和多个感兴趣区域（RoI）被输入到全卷积网络中。每个RoI被池化到固定大小的特征图中，然后通过全连接层（FC）映射到特征向量。网络对于每个RoI具有两个输出向量：Softmax概率和每类检测框回归偏移量。该架构是使用多任务丢失端到端训练的。</p>
<p>Fast R-CNN网络将整个图像和一组候选框作为输入。网络首先使用几个卷积层（conv）和最大池化层来处理整个图像，以产生卷积特征图。然后，对于每个候选框，RoI池化层从特征图中提取固定长度的特征向量。每个特征向量被送入一系列全连接（fc）层中，其最终分支成两个同级输出层 ：一个输出K<br>个类别加上1个背景类别的Softmax概率估计，另一个为K个类别的每一个类别输出四个实数值。每组4个值表示K个类别的一个类别的检测框位置的修正。</p>
<h3 id="RoI池化层"><a href="#RoI池化层" class="headerlink" title="RoI池化层"></a>RoI池化层</h3><p>RoI池化层使用最大池化将任何有效的RoI内的特征转换成具有H×W（例如，7×7）的固定空间范围的小特征图，其中H和W是层的超参数，独立于任何特定的RoI。在本文中，RoI是卷积特征图中的一个矩形窗口。 每个RoI由指定其左上角(r,c)及其高度和宽度(h,w)的四元组(r,c,h,w)定义。</p>
<p>RoI最大池化通过将大小为h×w的RoI窗口分割成H×W个网格，子窗口大小约为h/H×w/W，然后对每个子窗口执行最大池化，并将输出合并到相应的输出网格单元中。同标准的最大池化一样，池化操作独立应用于每个特征图通道。RoI层只是SPPnets<sup>5</sup>中使用的空间金字塔池层的特殊情况，其只有一个金字塔层。 我们使用<sup>5</sup>中给出的池化子窗口计算方法。</p>
<h3 id="从预训练网络初始化"><a href="#从预训练网络初始化" class="headerlink" title="从预训练网络初始化"></a>从预训练网络初始化</h3><p>我们实验了三个预训练的ImageNet<sup>9</sup>网络，每个网络有五个最大池化层和五到十三个卷积层（网络详细信息，请参见后文实验配置）。当预训练网络初始化fast R-CNN网络时，其经历三个变换。</p>
<p>首先，最后的最大池化层由RoI池层代替，其将H和W设置为与网络的第一个全连接层兼容的配置（例如，对于VGG16，H=W=7）。</p>
<p>然后，网络的最后一格全连接层和Softmax（其被训练用于1000类ImageNet分类）被替换为前面描述的两个同级层（全连接层和K+1个类别的Softmax以及类别特定的检测框回归）。</p>
<p>最后，网络被修改为采用两个数据输入：图像的列表和这些图像中的RoI的列表。</p>
<h3 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h3><p>用反向传播训练所有网络权重是Fast R-CNN的重要能力。首先，让我们阐明为什么SPPnet无法更新低于空间金字塔池化层的权重。</p>
<p>根本原因是当每个训练样本（即RoI）来自不同的图像时，通过SPP层的反向传播是非常低效的，这正是训练R-CNN和SPPnet网络的方法。低效的部分是因为每个RoI可能具有非常大的感受野，通常跨越整个输入图像。由于正向传播必须处理整个感受野，训练输入很大（通常是整个图像）。</p>
<p>我们提出了一种更有效的训练方法，利用训练期间的特征共享。在Fast RCNN网络训练中，随机梯度下降（SGD）的小批量是被分层采样的，首先采样N个图像，然后从每个图像采样R/N个 RoI。关键的是，来自同一图像的RoI在向前和向后传播中共享计算和内存。减小N，就减少了小批量的计算。例如，当N=2和R=128时，得到的训练方案比从128幅不同的图采样一个RoI（即R-CNN和SPPnet的策略）快64倍。</p>
<p>这个策略的一个令人担心的问题是它可能导致训练收敛变慢，因为来自相同图像的RoI是相关的。这个问题似乎在实际情况下并不存在，当N=2和R=128时，我们使用比R-CNN更少的SGD迭代就获得了良好的结果。</p>
<p>除了分层采样，Fast R-CNN使用了一个精细的训练过程，在微调阶段联合优化Softmax分类器和检测框回归，而不是分别在三个独立的阶段训练softmax分类器，SVM和回归器<sup>9 11</sup>。 下面将详细描述该过程（损失，小批量采样策略，通过RoI池化层的反向传播和SGD超参数）。</p>
<p><strong>多任务损失</strong>。Fast R-CNN网络具有两个同级输出层。 第一个输出在K+1个类别上的离散概率分布（每个RoI），$p=(p_0,…,p_K)$。 通常，通过全连接层的K+1个输出上的Softmax来计算p。第二个输出层输出检测框回归偏移,$t^k=(t^k_x,t^k_y,t^k_w,t^k_h)$，对于由k索引的K个类别中的每一个。 我们使用<sup>3</sup>中给出的$t^k$的参数化，其中$t^k$指定相对于候选框的尺度不变转换和对数空间高度/宽度移位。<br>每个训练的RoI用类真值u和检测框回归目标真值v标记。我们对每个标记的RoI使用多任务损失L以联合训练分类和检测框回归： </p>
<script type="math/tex; mode=display">L(p,u,t^u,v)=L_{cls}(p,u)+λ[u≥1]L_{loc}(t^u,v)   (1)</script><p>其中$L_{cls}(p,u)=-logp_u$，是类真值u的log损失。<br>对于类真值u，第二个损失$L_{loc}$是定义在检测框回归目标真值元组$u,v=(v_x,v_y,v_w,v_h)$和预测元组$tu=(t^u_x,t^u_y,t^u_w,t^u_h)$上的损失。 Iverson括号指示函数[u≥1]当u≥1的时候为值1，否则为0。按照惯例，背景类标记为u=0。对于背景RoI，没有检测框真值的概念，因此$L_{loc}$被忽略。对于检测框回归，我们使用损失 </p>
<script type="math/tex; mode=display">L_{loc}(t^u,v)=∑_{i∈{x,y,w,h}}smoothL_1(t^u_i-v_i)(2)</script><p>其中： </p>
<script type="math/tex; mode=display">
smooth_{L_1}(x)=
\begin{cases}
0.5x^2,if|x|<1 \\
|x|-0.5, otherwise\\
\end{cases}
(3)</script><p>是鲁棒的L1损失，对于异常值比在R-CNN和SPPnet中使用的L2损失更不敏感。当回归目标无界时，具有L2损失的训练可能需要仔细调整学习速率，以防止爆炸梯度。公式(3)消除了这种灵敏度。</p>
<p>公式(1)中的超参数λ控制两个任务损失之间的平衡。我们将回归目标真值$v_i$归一化为具有零均值和单位方差。所有实验都使用λ=1。</p>
<p>我们注意到<sup>10</sup>使用相关损失来训练一个类别无关的目标候选网络。 与我们的方法不同的是<sup>10</sup>倡导一个分离定位和分类的双网络系统。OverFeat<sup>4</sup>，R-CNN<sup>3</sup>和SPPnet<sup>5</sup>也训练分类器和检测框定位器，但是这些方法使用逐级训练，这对于Fast RCNN来说不是最好的选择。</p>
<p><strong>小批量采样</strong>。在微调期间，每个SGD的小批量由N=2个图像构成，均匀地随机选择（如通常的做法，我们实际上迭代数据集的排列）。 我们使用大小为R=128的小批量，从每个图像采样64个RoI。 如在<sup>3</sup>中，我们从候选框中获取25％的RoI，这些候选框与检测框真值的IoU至少为0.5。 这些RoI只包括用前景对象类标记的样本，即u≥1。 剩余的RoI从候选框中采样，该候选框与检测框真值的最大IoU在区间[0.1,0.5)<sup>5</sup>。 这些是背景样本，并用u=0标记。0.1的阈值下限似乎充当难负样本重训练的启发式算法<sup>11</sup>。 在训练期间，图像以概率0.5水平翻转。不使用其他数据增强。<br><strong>通过RoI池化层的反向传播</strong>。反向传播通过RoI池化层。为了清楚起见，我们假设每个小批量(N=1)只有一个图像，扩展到N&gt;1是显而易见的，因为前向传播独立地处理所有图像。</p>
<p>令xi∈ℝ是到RoI池化层的第i个激活输入，并且令$y_{rj}$是来自第r个RoI层的第j个输出。RoI池化层计算$y_{rj}=x_{i<em>(r,j)}$，其中$y_{rj}=x_{i</em>(r,j)}=argmax_{i’∈R(r,j)}x_{i’}$。$R(r,j)$是输出单元$y_rj$最大池化的子窗口中的输入的索引集合。单个$x_i$可以被分配给几个不同的输出$y_{rj}$。</p>
<p>RoI池化层反向传播函数通过遵循argmax switches来计算关于每个输入变量$x_i$的损失函数的偏导数： </p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial x_i} = Σ_{r}Σ_{j}[i=i^*(r,j)]\frac{\partial L}{\partial y_{rj}}(4)</script><p>换句话说，对于每个小批量RoI r和对于每个池化输出单元$y_{rj}$，如果i是$y_{rj}$通过最大池化选择的argmax，则将这个偏导数$\frac{\partial L}{\partial y_{rj}}$积累下来。在反向传播中，偏导数$\frac{\partial L}{\partial y_{rj}}$已经由RoI池化层顶部的层的反向传播函数计算。</p>
<p><strong>SGD超参数</strong>。用于Softmax分类和检测框回归的全连接层的权重分别使用具有方差0.01和0.001的零均值高斯分布初始化。偏置初始化为0。所有层的权重学习率为1倍的全局学习率，偏置为2倍的全局学习率，全局学习率为0.001。 当对VOC07或VOC12 trainval训练时，我们运行SGD进行30k次小批量迭代，然后将学习率降低到0.0001，再训练10k次迭代。当我们训练更大的数据集，我们运行SGD更多的迭代，如下文所述。 使用0.9的动量和0.0005的参数衰减（权重和偏置）。</p>
<h3 id="尺度不变性"><a href="#尺度不变性" class="headerlink" title="尺度不变性"></a>尺度不变性</h3><p>我们探索两种实现尺度不变对象检测的方法：（1）通过“brute force”学习和（2）通过使用图像金字塔。 这些策略遵循<sup>5</sup>中的两种方法。 在“brute force”方法中，在训练和测试期间以预定义的像素大小处理每个图像。网络必须直接从训练数据学习尺度不变性目标检测。</p>
<p>相反，多尺度方法通过图像金字塔向网络提供近似尺度不变性。 在测试时，图像金字塔用于大致缩放-规范化每个候选框。 在多尺度训练期间，我们在每次图像采样时随机采样金字塔尺度，遵循<sup>5</sup>，作为数据增强的形式。由于GPU内存限制，我们只对较小的网络进行多尺度训练。</p>
<h2 id="Fast-R-CNN检测"><a href="#Fast-R-CNN检测" class="headerlink" title="Fast R-CNN检测"></a>Fast R-CNN检测</h2><p>一旦Fast R-CNN网络被微调完毕，检测相当于运行前向传播（假设候选框是预先计算的）。网络将图像（或图像金字塔，编码为图像列表）和待计算概率的R个候选框的列表作为输入。在测试的时候，R通常在2000左右，虽然我们将考虑将它变大（约45k）的情况。当使用图像金字塔时，每个RoI被缩放，使其最接近<sup>5</sup>中的$224^2$个像素。</p>
<p>对于每个测试的RoI r，正向传播输出类别后验概率分布p和相对于r的预测的检测框框偏移集合（K个类别中的每一个获得其自己的精细检测框预测）。我们使用估计的概率$Pr(class=k|r)≜p_k$为每个对象类别k分配r的检测置信度。然后，我们使用R-CNN算法的设置和对每个类别独立执行非最大抑制<sup>3</sup>。</p>
<h3 id="使用截断的SVD来进行更快的检测"><a href="#使用截断的SVD来进行更快的检测" class="headerlink" title="使用截断的SVD来进行更快的检测"></a>使用截断的SVD来进行更快的检测</h3><p>对于整体图像分类，与卷积层相比，计算全连接层花费的时间较小。相反，为了检测，要处理的RoI的数量很大，并且接近一半的正向传递时间用于计算全连接层（参见图2）。大的全连接层容易通过用截短的SVD压缩来加速<sup>12 13</sup>。</p>
<p>在这种技术中，层的u×v权重矩阵W通过SVD被近似分解为：</p>
<script type="math/tex; mode=display">W≈UΣ_tV^T(5)</script><p>在这种分解中，U是一个u×t的矩阵，包括W的前t个左奇异向量，Σt是t×t对角矩阵，其包含W的前t个奇异值，并且V是v×t矩阵，包括W的前t个右奇异向量。截断SVD将参数计数从uv减少到t(u+v)个，如果t远小于min(u,v)，则SVD可能是重要的。 为了压缩网络，对应于W的单个全连接层由两个全连接层替代，在它们之间没有非线性。这些层中的第一层使用权重矩阵$Σ_tV^T$（没有偏置），并且第二层使用U（其中原始偏差与W相关联）。当RoI的数量大时，这种简单的压缩方法给出良好的加速。</p>
<h2 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h2><p>三个主要结果支持本文的贡献：</p>
<ul>
<li>VOC07，2010和2012的最高的mAP。</li>
<li>相比R-CNN，SPPnet，快速训练和测试。</li>
<li>在VGG16中微调卷积层改善了mAP。</li>
</ul>
<h3 id="实验配置"><a href="#实验配置" class="headerlink" title="实验配置"></a>实验配置</h3><p>我们的实验使用了三个经过预训练的ImageNet网络模型，这些模型可以在线获得(<a href="https://github.com/BVLC/caffe/wiki/Model-Zoo)。第一个是来自R-CNN3的CaffeNet（实质上是AlexNet1）。" target="_blank" rel="noopener">https://github.com/BVLC/caffe/wiki/Model-Zoo)。第一个是来自R-CNN3的CaffeNet（实质上是AlexNet1）。</a> 我们将这个CaffeNet称为模型S，即小模型。第二网络是来自14的VGG_CNN_M_1024，其具有与S相同的深度，但是更宽。 我们把这个网络模型称为M，即中等模型。最后一个网络是来自15的非常深的VGG16模型。由于这个模型是最大的，我们称之为L。在本节中，所有实验都使用单尺度训练和测试（s=600<br>，详见尺度不变性：暴力或精细？）。</p>
<h3 id="VOC-2010和2012数据集结果"><a href="#VOC-2010和2012数据集结果" class="headerlink" title="VOC 2010和2012数据集结果"></a>VOC 2010和2012数据集结果</h3><p><img src="/2018/07/22/fastrcnn_translation/2.png" alt="图2"><br>表2. VOC 2010测试检测平均精度（％）。 BabyLearning使用基于16的网络。 所有其他方法使用VGG16。训练集：12：VOC12 trainval，Prop.：专有数据集，12+seg：12具有分段注释，07++12：VOC07 trainval，VOC07测试和VOC12 trainval的联合。<br><img src="/2018/07/22/fastrcnn_translation/3.png" alt="图3"><br>表3. VOC 2012测试检测平均精度（％）。 BabyLearning和NUS_NIN_c2000使用基于16的网络。 所有其他方法使用VGG16。训练设置：见表2，Unk.：未知。</p>
<p>上表（表2，表3）所示，在这些数据集上，我们比较Fast R-CNN（简称FRCN）和公共排行榜中comp4（外部数据）上的主流方法（<a href="http://host.robots.ox.ac.uk:8080/leaderboard" target="_blank" rel="noopener">http://host.robots.ox.ac.uk:8080/leaderboard</a> ，访问时间是2015.4.18）。对于NUS_NIN_c2000和BabyLearning方法，目前没有其架构的确切信息，它们是Network-in-Network的变体16。所有其他方法从相同的预训练VGG16网络初始化。</p>
<p>Fast R-CNN在VOC12上获得最高结果，mAP为65.7％（加上额外数据为68.4％）。它也比其他方法快两个数量级，这些方法都基于比较“慢”的R-CNN网络。在VOC10上，SegDeepM<sup>6</sup>获得了比Fast R-CNN更高的mAP（67.2％对比66.1％）。SegDeepM使用VOC12 trainval训练集训练并添加了分割的标注，它被设计为通过使用马尔可夫随机场推理R-CNN检测和来自O2P<sup>17</sup>的语义分割方法的分割来提高R-CNN精度。Fast R-CNN可以替换SegDeepM中使用的R-CNN，这可以导致更好的结果。当使用放大的07++12训练集（见表2标题）时，Fast R-CNN的mAP增加到68.8％，超过SegDeepM。</p>
<h3 id="VOC-2007数据集上的结果"><a href="#VOC-2007数据集上的结果" class="headerlink" title="VOC 2007数据集上的结果"></a>VOC 2007数据集上的结果</h3><p>在VOC07数据集上，我们比较Fast R-CNN与R-CNN和SPPnet的mAP。 所有方法从相同的预训练VGG16网络开始，并使用边界框回归。 VGG16 SPPnet结果由5的作者提供。SPPnet在训练和测试期间使用五个尺度。Fast R-CNN对SPPnet的改进说明，即使Fast R-CNN使用单个尺度训练和测试，卷积层微调在mAP中提供了大的改进（从63.1％到66.9％）。R-CNN的mAP为66.0％。 作为次要点，SPPnet在PASCAL中没有使用被标记为“困难”的样本进行训练。 除去这些样本，Fast R-CNN 的mAP为68.1％。 所有其他实验都使用被标记为“困难”的样本。</p>
<h3 id="测试和训练时间"><a href="#测试和训练时间" class="headerlink" title="测试和训练时间"></a>测试和训练时间</h3><p><img src="/2018/07/22/fastrcnn_translation/4.png" alt="图4"><br>表4. Fast RCNN，R-CNN和SPPnet中相同模型之间的运行时间比较。Fast R-CNN使用单尺度模式。SPPnet使用<sup>5</sup>中指定的五个尺度，由5的作者提供在Nvidia K40 GPU上的测量时间。</p>
<p>快速的训练和测试是我们的第二个主要成果。表4比较了Fast RCNN，R-CNN和SPPnet之间的训练时间（小时），测试速率（每秒图像数）和VOC07上的mAP。对于VGG16，没有截断SVD的Fast R-CNN处理图像比R-CNN快146倍，有截断SVD的R-CNN快213倍。训练时间减少9倍，从84小时减少到9.5小时。与SPPnet相比，没有截断SVD的Fast RCNN训练VGG16网络比SPPnet快2.7倍（9.5小时对25.5小时），测试时间快7倍，有截断SVD的Fast RCNN比的SPPnet快10倍。 Fast R-CNN还不需要数百GB的磁盘存储，因为它不缓存特征。</p>
<p><strong>截断SVD</strong>。截断的SVD可以将检测时间减少30％以上，同时在mAP中只有很小（0.3个百分点）的下降，并且无需在模型压缩后执行额外的微调。<br><img src="/2018/07/22/fastrcnn_translation/5.png" alt="图5"><br>图2. 截断SVD之前和之后VGG16的时间分布。在SVD之前，完全连接的层fc6和fc7需要45％的时间。</p>
<p>图2示出了如何使用来自VGG16的fc6层中的25088×4096<br>矩阵的顶部1024个奇异值和来自fc7层的4096×4096矩阵的顶部256个奇异值减少运行时间，而在mAP中几乎没有损失。如果在压缩之后再次微调，则可以在mAP中具有更小的下降的情况下进一步加速。</p>
<h3 id="微调哪些层"><a href="#微调哪些层" class="headerlink" title="微调哪些层"></a>微调哪些层</h3><p>对于在SPPnet论文5中考虑的不太深的网络，仅微调全连接层似乎足以获得良好的精度。我们假设这个结果不适用于非常深的网络。为了验证微调卷积层对于VGG16的重要性，我们使用Fast R-CNN微调，但冻结十三个卷积层，以便只有全连接层学习。这种消融模拟单尺度SPPnet训练，将mAP从66.9％降低到61.4％（表5）。这个实验验证了我们的假设：通过RoI池化层的训练对于非常深的网是重要的。<br><img src="/2018/07/22/fastrcnn_translation/6.png" alt="图6"><br>表5. 限制哪些层对VGG16进行微调产生的影响。微调≥fc6模拟单尺度SPPnet训练算法5。 SPPnet L是使用五个尺度，以显著（7倍）的速度成本获得的结果。</p>
<p>这是否意味着所有卷积层应该微调？没有。在较小的网络（S和M）中，我们发现conv1（第一个卷积层）是通用的和任务独立的（一个众所周知的事实1）。允许conv1学习或不学习，对mAP没有很有意义的影响。对于VGG16，我们发现只需要更新conv3_1及以上（13个卷积层中的9个）的层。这种观察是实用的：（1）从conv2_1更新使训练变慢1.3倍（12.5小时对比9.5小时）和（2）从conv1_1更新GPU内存不够用。当从conv2_1学习时mAP仅为增加0.3个点（表5，最后一列）。 所有Fast R-CNN在本文中结果都使用VGG16微调层conv3_1及以上的层，所有实验用模型S和M微调层conv2及以上的层。</p>
<h2 id="设计评估"><a href="#设计评估" class="headerlink" title="设计评估"></a>设计评估</h2><p>我们通过实验来了解Fast RCNN与R-CNN和SPPnet的比较，以及评估设计决策。按照最佳实践，我们在PASCAL VOC07数据集上进行了这些实验。</p>
<h3 id="多任务训练有用吗？"><a href="#多任务训练有用吗？" class="headerlink" title="多任务训练有用吗？"></a>多任务训练有用吗？</h3><p>多任务训练是方便的，因为它避免管理顺序训练任务的流水线。但它也有可能改善结果，因为任务通过共享的表示（ConvNet）18相互影响。多任务训练能提高Fast R-CNN中的目标检测精度吗？</p>
<p>为了测试这个问题，我们训练仅使用公式(1)中的分类损失$L_{cls}$（即设置λ=0）的基准网络。这些基线是表6中每组的第一列。请注意，这些模型没有检测框回归。接下来（每组的第二列），是我们采用多任务损失（公式(1)，λ=1）训练的网络，但是我们在测试时禁用检测框回归。这隔离了网络的分类准确性，并允许与基准网络的apple to apple的比较。</p>
<p>在所有三个网络中，我们观察到多任务训练相对于单独的分类训练提高了纯分类精度。改进范围从+0.8到+1.1 个mAP点，显示了多任务学习的一致的积极效果。</p>
<p>最后，我们采用基线模型（仅使用分类损失进行训练），加上检测回归层，并使用$L_{loc}$训练它们，同时保持所有其他网络参数冻结。每组中的第三列显示了这种逐级训练方案的结果：mAP相对于第一列改进，但逐级训练表现不如多任务训练（每组第四列）。<br><img src="/2018/07/22/fastrcnn_translation/7.png" alt="图7"><br>表6. 多任务训练（每组第四列）改进了分段训练（每组第三列）的mAP。</p>
<h3 id="尺度不变性：暴力或精细？"><a href="#尺度不变性：暴力或精细？" class="headerlink" title="尺度不变性：暴力或精细？"></a>尺度不变性：暴力或精细？</h3><p>我们比较两个策略实现尺度不变物体检测：暴力学习（单尺度）和图像金字塔（多尺度）。在任一情况下，我们将图像的尺度s定义为其最短边的长度。</p>
<p>所有单尺度实验使用s=600像素，对于一些图像，s可以小于600，因为我们保持横纵比缩放图像，并限制其最长边为1000像素。选择这些值使得VGG16在微调期间不至于GPU内存不足。较小的模型占用显存更少，所以可受益于较大的s值。然而，每个模型的优化不是我们的主要的关注点。我们注意到PASCAL图像是384×473像素的，因此单尺度设置通常以1.6倍的倍数上采样图像。因此，RoI池化层的平均有效步进为约10像素。</p>
<p>在多尺度设置中，我们使用5中指定的相同的五个尺度（s∈{480,576,688,864,1200}）以方便与SPPnet进行比较。但是，我们以2000像素为上限，以避免GPU内存不足。</p>
<p>表7显示了当使用一个或五个尺度进行训练和测试时的模型S和M的结果。也许在5中最令人惊讶的结果是单尺度检测几乎与多尺度检测一样好。我们的研究结果能证明他们的结果：深度卷积网络擅长直接学习尺度不变性。多尺度方法消耗大量的计算时间仅带来了很小的mAP增加（表7）。在VGG16（模型L）的情况下，我们受限于实施细节仅能使用单个尺度。然而，它得到了66.9％的mAP，略高于R-CNN的66.0％19，尽管R-CNN在每个候选区域被缩放为规范大小，在意义上使用了“无限”尺度。</p>
<p>由于单尺度处理提供速度和精度之间的最佳折衷，特别是对于非常深的模型，本小节以外的所有实验使用单尺度训练和测试，s=600像素。</p>
<p><img src="/2018/07/22/fastrcnn_translation/8.png" alt="图8"><br>表7. 多尺度与单尺度。SPPnet ZF（类似于模型S）的结果来自5。 具有单尺度的较大网络提供最佳的速度/精度平衡。（L在我们的实现中不能使用多尺度，因为GPU内存限制。）</p>
<h3 id="SVM分类是否优于Softmax？"><a href="#SVM分类是否优于Softmax？" class="headerlink" title="SVM分类是否优于Softmax？"></a>SVM分类是否优于Softmax？</h3><p>Fast R-CNN在微调期间使用softmax分类器学习，而不是如在R-CNN和SPPnet中训练线性SVM。为了理解这种选择的影响，我们在Fast R-CNN中实施了具有难负采样重训练的SVM训练。我们使用与R-CNN中相同的训练算法和超参数。<br><img src="/2018/07/22/fastrcnn_translation/9.png" alt="图9"><br>表8. 用Softmax的Fast R-CNN对比用SVM的Fast RCNN（VOC07 mAP）。</p>
<p>对于所有三个网络，Softmax略优于SVM，mAP分别提高了0.1和0.8个点。这种效应很小，但是它表明与先前的多级训练方法相比，“一次性”微调是足够的。我们注意到，Softmax，不像SVM那样，在分类RoI时引入类之间的竞争。</p>
<h3 id="更多的候选区域更好吗？"><a href="#更多的候选区域更好吗？" class="headerlink" title="更多的候选区域更好吗？"></a>更多的候选区域更好吗？</h3><p>存在（广义地）两种类型的目标检测器：使用候选区域的稀疏集合（例如，选择性搜索<sup>21</sup>）和使用密集集合（例如DPM<sup>11</sup>）。分类稀疏提议是级联的一种类型<sup>22</sup>，其中提议机制首先拒绝大量候选者，让分类器来评估留下的小集合。当应用于DPM检测时，该级联提高了检测精度<sup>21</sup>。我们发现提案分类器级联也提高了Fast R-CNN的精度。</p>
<p>使用选择性搜索的质量模式，我们扫描每个图像1k到10k个候选框，每次重新训练和重新测试模型M.如果候选框纯粹扮演计算的角色，增加每个图像的候选框数量不应该损害mAP。<br><img src="/2018/07/22/fastrcnn_translation/10.png" alt="图10"><br>图3. 各种候选区域方案的VOC07测试mAP和AR。</p>
<p>我们发现mAP上升，然后随着候选区域计数增加而略微下降（图3，实线蓝线）。这个实验表明，用更多的候选区域没有帮助，甚至稍微有点伤害准确性。</p>
<p>如果不实际运行实验，这个结果很难预测。用于测量候选区域质量的最先进的技术是平均召回率(AR)23。当对每个图像使用固定数量的候选区域时，AR与使用R-CNN的几种候选区域方法良好地相关。图3示出了AR（实线红线）与mAP不相关，因为每个图像的候选区域数量是变化的。AR必须小心使用，由于更多的候选区域更高的AR并不意味着mAP会增加。幸运的是，使用模型M的训练和测试需要不到2.5小时。因此，Fast R-CNN能够高效地，直接地评估目标候选区域mAP，这优于代理度量。</p>
<p>我们还调查Fast R-CNN当使用密集生成框（在缩放，位置和宽高比上），大约45k个框/图像。这个密集集足够丰富，当每个选择性搜索框被其最近（IoU）密集框替换时，mAP只降低1个点（到57.7％，图3，蓝色三角形）。</p>
<p>密集框的统计数据与选择性搜索框的统计数据不同。从2k个选择性搜索框开始，我们在添加1000×{2,4,6,8,10,32,45}的随机样本密集框时测试mAP。对于每个实验，我们重新训练和重新测试模型M。当添加这些密集框时，mAP比添加更多选择性搜索框时下降得更强，最终达到53.0％。</p>
<p>我们还训练和测试Fast R-CNN只使用密集框（45k/图像）。此设置的mAP为52.9％（蓝色菱形）。最后，我们检查是否需要使用难样本重训练的SVM来处理密集框分布。 SVM做得更糟：49.3％（蓝色圆圈）。</p>
<h3 id="MS-COCO初步结果"><a href="#MS-COCO初步结果" class="headerlink" title="MS COCO初步结果"></a>MS COCO初步结果</h3><p>我们将fast R-CNN（使用VGG16）应用于MS COCO数据集<sup>24</sup>，以建立初步基线。我们对80k图像训练集进行了240k次迭代训练，并使用评估服务器对“test-dev”集进行评估。 PASCAL标准下的mAP为35.9％;。新的COCO标准下的AP（也平均）为19.7％。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文提出Fast R-CNN，一个对R-CNN和SPPnet干净，快速的更新。 除了报告目前的检测结果之外，我们还提供了详细的实验，希望提供新的见解。 特别值得注意的是，稀疏目标候选区域似乎提高了检测器的质量。 过去探索这个问题过于昂贵（在时间上），但Fast R-CNN使其变得可能。当然，可能存在允许密集盒执行以及稀疏候选框的尚未发现的技术。这样的方法如果被开发，可以帮助进一步加速目标检测。</p>
<p>致谢：感谢Kaiming He，Larry Zitnick和Piotr Dollár的帮助和鼓励。</p>
<h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><pre><code>1. A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012. 

2. Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Comp., 1989. 

3. R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 

4. P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In ICLR, 2014.

5. K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. 

6. Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler. segDeepM: Exploiting segmentation and context in deep neural networks for object detection. In CVPR, 2015.

7. S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR, 2006. 

8. Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proc. of the ACM International Conf. on Multimedia, 2014. 

9. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009. 

10. D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks. In CVPR, 2014. 

11. P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. TPAMI, 2010.

12. E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In NIPS, 2014.

13. J. Xue, J. Li, and Y. Gong. Restructuring of deep neural network acoustic models with singular value decomposition. In Interspeech, 2013. 

14. K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: Delving deep into convolutional nets. In BMVC, 2014. 

15. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 

16. M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR, 2014. 

17. J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Semantic segmentation with second-order pooling. In ECCV, 2012. 

18. R. Caruana. Multitask learning. Machine learning, 28(1), 1997. 

19. R. Girshick, J. Donahue, T. Darrell, and J. Malik. Region-based convolutional networks for accurate object detection and segmentation. TPAMI, 2015. 

20. X. Zhu, C. Vondrick, D. Ramanan, and C. Fowlkes. Do we need more training data or better models for object detection? In BMVC, 2012. 

21. J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recognition. IJCV, 2013. 

22. P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In CVPR, 2001. 

23. J. H. Hosang, R. Benenson, P. Dollár, and B. Schiele. What makes for effective detection proposals? arXiv preprint arXiv:1502.05082, 2015. 

24. T. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: common objects in context. arXiv e-prints, arXiv:1405.0312 [cs.CV], 2014. 
</code></pre>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          
            <a href="/tags/translation/" rel="tag"># translation</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/21/rcnn_translation/" rel="next" title="📜论文翻译 Rich feature hierarchies for accurate object detection and semantic segmentation (用于精确物体定位和语义分割的丰富特征层次结构-2014)">
                <i class="fa fa-chevron-left"></i> 📜论文翻译 Rich feature hierarchies for accurate object detection and semantic segmentation (用于精确物体定位和语义分割的丰富特征层次结构-2014)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/24/fasterrcnn_translation/" rel="prev" title="📜论文翻译 Faster R-CNN：利用区域提案网络实现实时目标检测">
                📜论文翻译 Faster R-CNN：利用区域提案网络实现实时目标检测 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="JL Ma" />
            
              <p class="site-author-name" itemprop="name">JL Ma</p>
              <p class="site-description motion-element" itemprop="description">simple but powerful</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#摘要"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#简介"><span class="nav-number">2.</span> <span class="nav-text">简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#R-CNN与SPPnet"><span class="nav-number">2.1.</span> <span class="nav-text">R-CNN与SPPnet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#贡献"><span class="nav-number">2.2.</span> <span class="nav-text">贡献</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fast-R-CNN架构与训练"><span class="nav-number">3.</span> <span class="nav-text">Fast R-CNN架构与训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RoI池化层"><span class="nav-number">3.1.</span> <span class="nav-text">RoI池化层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从预训练网络初始化"><span class="nav-number">3.2.</span> <span class="nav-text">从预训练网络初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#微调"><span class="nav-number">3.3.</span> <span class="nav-text">微调</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#尺度不变性"><span class="nav-number">3.4.</span> <span class="nav-text">尺度不变性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fast-R-CNN检测"><span class="nav-number">4.</span> <span class="nav-text">Fast R-CNN检测</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用截断的SVD来进行更快的检测"><span class="nav-number">4.1.</span> <span class="nav-text">使用截断的SVD来进行更快的检测</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#主要结果"><span class="nav-number">5.</span> <span class="nav-text">主要结果</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#实验配置"><span class="nav-number">5.1.</span> <span class="nav-text">实验配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VOC-2010和2012数据集结果"><span class="nav-number">5.2.</span> <span class="nav-text">VOC 2010和2012数据集结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VOC-2007数据集上的结果"><span class="nav-number">5.3.</span> <span class="nav-text">VOC 2007数据集上的结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#测试和训练时间"><span class="nav-number">5.4.</span> <span class="nav-text">测试和训练时间</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#微调哪些层"><span class="nav-number">5.5.</span> <span class="nav-text">微调哪些层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#设计评估"><span class="nav-number">6.</span> <span class="nav-text">设计评估</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#多任务训练有用吗？"><span class="nav-number">6.1.</span> <span class="nav-text">多任务训练有用吗？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#尺度不变性：暴力或精细？"><span class="nav-number">6.2.</span> <span class="nav-text">尺度不变性：暴力或精细？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM分类是否优于Softmax？"><span class="nav-number">6.3.</span> <span class="nav-text">SVM分类是否优于Softmax？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#更多的候选区域更好吗？"><span class="nav-number">6.4.</span> <span class="nav-text">更多的候选区域更好吗？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MS-COCO初步结果"><span class="nav-number">6.5.</span> <span class="nav-text">MS COCO初步结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#结论"><span class="nav-number">7.</span> <span class="nav-text">结论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献："><span class="nav-number">8.</span> <span class="nav-text">参考文献：</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JL Ma</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> total vistors
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      people
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> total read
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      times
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
